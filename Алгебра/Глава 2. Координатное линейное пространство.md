# n-мерные векторы и действия над ними
Пусть $k$ - некоторое фиксированное числовое поле, которое называется основным. Элементы поля $k$ будем обозначать греческими буквами $\alpha, \beta, \gamma, \alpha_1, \alpha_2,...$ - скалярные элементы.

***Определение***:
$n$-мерным вектором надо полем $k$ называется любая упорядоченная совокупность $n$-чисел поля $k$.

Обозначать векторы будем маленькими латинскими буквами $a, b, c, a_1, a_2, ...$
$a = (\alpha_1, \alpha_2, ..., \alpha_n), \ b = (\beta_1, \beta_2, ..., \beta_n)$ - упорядоченная совокупность $n$-чисел поля $k$. Множество всех $n$-мерных векторов над полем $k$ будем обозначать 
$$
k^n = \{(\alpha_1, \alpha_2, ..., \alpha_n) | \alpha_i \in k\}
$$

***Определение***:
Координатами вектора  $a$ называются числа, из которых состоит этот вектор $a$.
$$
\alpha_1, \ \alpha_2, ..., \ \alpha_n
$$

***Определение***:
Два вектора $a = (\alpha_1, ..., \alpha_n)$ и $b = (\beta_1, ..., \beta_n)$ называются равными, если равны все их соответственные координаты, то есть 
$$
(\forall \ 1 \leq i \leq n) \ \ \alpha_i=\beta_i
$$

***Определение***:
Суммой двух векторов $a \ и \ b$ называется вектор $a+b$, координаты которого равны суммам соответственных координат векторов $a \ и \ b$.
$$
a + b = (\alpha_1 + \beta_1, \alpha_2 + \beta_2, ..., \alpha_n + \beta_n).
$$

***Определение***:
Произведением вектора $a$ на число $\alpha$ называется вектор $a\alpha$, координаты которого равны соответственным координатам вектора $a$, умноженных на число $\alpha$.
$$
\alpha a = (\alpha a_1, \alpha a_2, ..., \alpha a_n).
$$

***Определение***:
Совокупность всех $n$-мерных векторов над полем $k$, рассмотренная вместе с определёнными на ней операциями сложение векторов и умножение вектором на число, называется $n$-мерным координатным (векторным) пространством.

***Теорема** (О свойствах операций в $k^n$)*:
Операции, определённые в $k^n$:
1. $a+b=b+a$ (коммутативность сложения векторов)
2. $a + (b + c) = (a + b) + c$ (ассоциативность сложения векторов)
3. $(\forall \ a, b \in k^n) \ (\exists x \in k^n) \ b + x = a$ (обратимость сложения векторов)
4. $\alpha(a + b) = \alpha a + \alpha b$ (дистрибутивность умножения относительно сложения векторов)
5. $(\alpha + \beta)a = \alpha a + \beta a$ (дистрибутивность умножения относительно сложения чисел)
6. $(\alpha \beta)a=\alpha(\beta a) = \beta (\alpha a)$ (ассоциативность умножения на число)
7. $1 * a = a$ (существование нейтрального элемента)

*Доказательство*:
1. $a + b = (\alpha_1 + \beta_1, \alpha_2 + \beta_2, ..., \alpha_n + \beta_n) = (\beta_1+\alpha_1, \beta_2+\alpha_2,...\beta_n+\alpha_n) = b+a$
2. -
3. Пусть $a=(\alpha_1, \alpha_2, ..., \alpha_n), \ b=(\beta_1, \beta_2, ..., \beta_n)$. Вектор $x$ будем искать в виде $(x_1, x_2, ..., x_n)$, где $x_1, x_2, ..., x_n$ - неизвестные координаты; Перепишем $b + x = a$ в координатной форме:
   $$
   (\beta_1+x_1, \beta_2+x_x,...,\beta_n+x_n)=(\alpha_1, \alpha_2, ..., \alpha_n) \ \ \ (\forall \ 1 \leq i \leq n) \ \beta_i+x_i=a_i
   $$
   Отсюда находим, что
   $$(\forall \ 1\leq i \leq n) \ \ x_i=\alpha_i-\beta_i$$
   Координаты вектора $x$ найдены. Следовательно,
   $$
   x = (\alpha_1 - \beta_1, \alpha_2 - \beta_2, ..., \alpha_n - \beta_n).
   $$
4. -
5. $$\begin{gathered}(\alpha + \beta)a = \\ =((\alpha + \beta)\alpha_1, (\alpha+\beta)\alpha_2, ..., (\alpha+\beta)\alpha_n)= \\=(\alpha\alpha_1+\beta\alpha_1, \alpha\alpha_2 + \beta\alpha_2, ..., \alpha\alpha_n+\beta\alpha_n)=\\=(\alpha\alpha_1, \alpha\alpha_2, ..., \alpha\alpha_n)+(\beta\alpha_1, \beta\alpha_2, ..., \beta\alpha_n)= \\ =\alpha a + \beta a.\end{gathered}$$

***Определение***:
Вектор $x$, удовлетворяющий уравнению $b + x = a$, называется разностью векторов $a$ и $b$ и обозначается $x = a - b$

***Следствия из теоремы***:
1. Роль нуля при сложении векторов играет нулевой вектор $0 = (0, 0, ..., 0)$.
   Действительно, $(\forall \ a \in k^n) \ \ a + 0 = (\alpha_1 + 0, \alpha_2 + 0, ..., \alpha_n + 0) = (\alpha_1, \alpha_2, ..., \alpha_n) = a$.
2. $(\forall \ a \in k^n) \ \ (\exists (-a) \in k^n) \ \ a + (-a) = 0$
   В самом деле, $(-a) = (-\alpha_1, -\alpha_2, ..., -\alpha_n).$ Вектор $(-a)$ называется противоположным вектором для вектора $a$.
3. Вектор $x$, удовлетворяющий уравнению $b+a=x$, может быть найден из условия $x=a+(-b)$.
   Действительно, $b + [a + (-b)] = b + [(-b) + a] = [b+(-b)] + a = 0 + a = a \ \ \rightarrow a - b = a + (-b)$
4. $(-\alpha)a = \alpha(-a) = -\alpha a$
5. $\alpha(a - b) = \alpha a - \alpha b$
6. $(\alpha - \beta) a = \alpha a - \beta a.$

---

# Линейно зависимые и линейно независимые системы векторов
***Определение***:
Говорят, что вектор $a$ является линейной комбинацией векторов $a_1, a_2 , ..., a_s$, если существуют скаляры  $\alpha_1, \alpha_2, ..., \alpha_s$, не все равные нулю, такие, что $a = \alpha_1 a_1 + \alpha_2 a_2 + ... + \alpha_s a_s$.

***Определение***:
Система векторов $a_1, a_2, ..., a_s$ называется линейно зависимой (ЛЗ), если существуют скаляры $\alpha_1, \alpha_2, ..., \alpha_s$, не все равные нулю, такие, что $\alpha_1 a_1 + \alpha_2 a_2 + ... + \alpha_s a_s = 0.$

Для ЛЗ систем существуют нетривиальные линейные комбинации, обращающиеся в ноль.

***Определение***:
Система векторов $a_1, a_2, ..., a_s$ называется линейно независимой (ЛНЗ), если из равенства $\alpha_1 a_1 + \alpha_2 + a_2 + ... + \alpha_s a_s = 0$ следует, что $\alpha_1 = \alpha_2 = ... = \alpha_s = 0.$

Для ЛНЗ систем в ноль может обращаться только тривиальные линейные комбинации этих векторов.

***Теорема** (критерий линейной зависимости)*:
Система векторов $a_1, a_2, ..., a_s, \ (s\geq 2)$ линейно зависима тогда и только тогда, когда хотя бы один вектор системы линейно выражается через остальные векторы этой системы векторов.

*Доказательство*:
1. Необходимость
   Пусть система векторов $a_1, a_2, ..., a_s$ является ЛЗ. По определению существуют скаляры $\alpha_1, \alpha_2, ..., \alpha_s$, не все равные нулю, такие, что $\alpha_1a_1 + \alpha_2a_2+ ... + \alpha_sa_s = 0$. Пусть, для определённости, $\alpha_1 \not= 0$, тогда 
   $$
   a_1 = (-\frac{\alpha_2}{\alpha_1})a_2 + ... + (-\frac{\alpha_s}{\alpha_1})a_s.
   $$
   Вектор $a_1$ является линейной комбинацией остальных векторов.
2. Достаточность
   Пусть, например, вектор $a_1$ линейно выражается через $a_2, ..., a_s$ $$
   a_1 = \alpha_2a_2 + ... + \alpha_sa_s \Rightarrow (-1)a_1 + \alpha_2a_2 + ... + \alpha_sa_s = 0.
   $$
   Это равенство указывает на то, что нетривиальная линейная комбинация векторов $a_1, a_2, ..., a_s$ равна нулю. По определению векторы $a_1, a_2, ..., a_s$ линейно зависимы.

*Свойства линейной зависимости*:
1. *Если какая-либо подсистема векторов является ЛЗ, то и вся система векторов тоже будет ЛЗ.*
   Действительно, пусть $a_1, a_2, ..., a_s$ вся система векторов и $a_1, a_2, ..., a_r \ (r\geq s)$ является ЛЗ. По определению существуют $\alpha_1, \alpha_2, ..., \alpha_r$ не все равные нулю, такие что
   $$
   \alpha_1a_1 + \alpha_2a_2 + ... + \alpha_ra_r = 0.
   $$
   Мы не нарушим равенство, если запишем
   $$
   \alpha_1a_1 + ... + \alpha_ra_r + 0 * a_{r+1} + ... + 0 * a_s = 0. 
   $$
   Видно, что нетривиальная линейная комбинация векторов $a_1, a_2, ..., a_s$ равна нулю. Это означает, что система векторов $a_1, a_2, ..., a_s$ является ЛЗ.

2. *Если система векторов является ЛНЗ, то и любая её подсистема также является ЛНЗ.*
   Действительно, допустим противное, то есть некоторая подсистема ЛНЗ системы оказалась ЛЗ. Тогда по свойству  1 и вся система будет ЛЗ, а это противоречит условию.
3. *Если система векторов $a_1, a_2, ..., a_s$ является ЛНЗ, а система векторов $a_1, a_2, ..., a_s, b$ является ЛЗ, то вектор $b$ является линейной комбинацией векторов $a_1, a_2, ..., a_s$.* 
   Действительно, так как $a_1, a_2, ..., a_s, b$ является ЛЗ, то существуют $\alpha_1, \alpha_2, ..., \alpha_s, \beta$, не все равные нулю, такие, что
   $$
   \alpha_1a_1 + \alpha_2a_2 + ... + \alpha_sa_s + \beta b = 0.
   $$
   Покажем, что $\beta \not=0$. Допустим противное, то есть $\beta = 0$. Тогда получим
   $$
\alpha_1a_1 + \alpha_2a_2 + ... + \alpha_sa_s = 0,
   $$
   где не все $\alpha_1, \alpha_2, ..., \alpha_s$ равны нулю. А это означает, что система векторов $a_1, a_2, ..., a_s$ является ЛЗ, что противоречит условию. Следовательно, $\beta \not= 0$, тогда вектор
   $$
   b = (-\frac{\alpha_1}{\beta})a_1 + ... + (-\frac{\alpha_s}{\beta})a_s.
   $$
   Видно, что вектор $b$ линейно выражается через $a_1, a_2, ..., a_s$, значит является линейной комбинацией этих векторов.

4. *Два вектора ЛЗ тогда и только тогда, когда они пропорциональны*
   1. Необходимость
      Пусть векторы $a_1 \ и \ a_2$ линейно зависимы. По определению это означает, что существуют скаляры $\alpha_1 \ и \ \alpha_2$, одновременно не равные нулю, такие, что $\alpha_1a_1 + \alpha_2a_2 = 0.$ Пусть, например, $\alpha_1 \not= 0$. Тогда из последнего равенства получаем
       $$
       a_1 = (-\frac{\alpha_2}{\alpha_1})a_2,
       $$
       то есть векторы $a_1 \ и \ a_2$ являются пропорциональными. 
    2. Достаточность
       Пусть векторы $a_1 \ и \ a_2$ являются пропорциональными. Это означает, что существует скаляр $k$ такой, что $a_1 = ka_2 \ (или \ a_2=ka_1)$. Получаем $(-1)a_1+ka_2 = 0.$ Нетривиальная линейная комбинация векторов $a_1 \ и \ a_2$ равна нулю. Следовательно, векторы $a_1 \ и \ a_2$ линейно зависимы.

5. *Система векторов, содержащая хотя бы два пропорциональных вектора или нулевой вектор, является ЛЗ.*

**Теорема** (основная теорема о линейной зависимости)
Пусть даны две системы векторов
$$
\begin{gathered}
a_1, a_2, ..., a_p \ (I) \\
b_1, b_2, ..., b_q \ (II)
\end{gathered}
$$
Если система векторов $(I)$ является ЛНЗ и каждый вектор системы $(I)$ линейно выражается через векторы системы $(II)$, то число векторов в системе $(I)$ не превосходит числа векторов системы $(II)$, то есть $p \leq q$.


*Доказательство*:
Доказательство проведём методом математической индукции по $q$.

Пусть $q = 1$. Надо показать, что $p \leq 1$. Имеем в системе $(II)$ вектор $b_1$. Допустим противное, то есть $p \geq 2$. Это означает, что в системе имеется по крайней мере два вектора. Они линейно выражаются через $b_1$.
$$
a_1 = \alpha_1b_1; \ \ \ a_2 = \alpha_1b_1 \ \ \ \alpha_1 \not= 0.
$$
Если бы $\alpha_1 = 0$, то $a_1=0$. Тогда система $(I)$ была бы ЛЗ (по свойству 3). Следовательно, $\alpha_1 \not= 0$. Тогда $b_1 = \frac{1}{\alpha_1}a_1$, подставим, получим $a_1 = \frac{\alpha_2}{\alpha_1}a_1.$ Векторы $a_1 \ и \ a_2$ пропорциональны, а это опять означало бы линейную зависимость системы $(I)$. Противоречие. $\Rightarrow$ $p \leq 1$. Для $q = 1$ теорема доказана.

Предположим, что теорема справедлива для случая, когда в системе $(II)$ содержится $q-1$ вектор. Докажем её справедливость для случая $q$ векторов в системе $(II)$. Запишем тот факт, что векторы системы $(I)$ линейно выражаются через векторы системы $(II)$.
$$
\begin{gathered}
a_1 = \alpha_{11}b_1 + ... + \alpha_{1 \ q-1}b_{q-1} + \alpha_{1q}b_q; \\
... \\
a_{p-1} = \alpha_{p-1 \ 1}b_1 + ... + \alpha_{p - 1 \ q-1}b_{q-1} + \alpha_{p-1 \ q}b_q; \\
a_p = \alpha_{p1}b_1 + ... + \alpha_{p \ q-1}b_{q-1} + \alpha_{pq}b_q
\end{gathered} \ \ \ \ (2.1)
$$
Рассмотрим два случая.
1. Все скаляры при векторе $b_q$ в равенствах $(2.1)$ равны нулю $\alpha_{1q} = ... = \alpha{p-1 \ q} = \alpha_{pq} = 0$. В этом случае ЛНЗ система $(I)$ $a_1, a_2, ..., a_p$ линейно выражается через векторы $b_1, b_2, ..., b_{q-1}$. Тогда по предположению индукции $p \leq q-1$. Тогда $p \leq q$.
2. Не все скаляры при векторе $b_q$ в равенствах $(2.1)$ равны нулю. Пусть, например, $\alpha_{pq} \not= 0$. С помощью последнего равенства в $(2.1)$ исключим вектор $b_q$ из первых $p - 1$ равенств. Для этого из $i$-го равенства $(1 \leq i \leq p - 1)$ мы вычитаем последнее равенство, умноженное на $\frac{\alpha_{iq}}{\alpha_{pq}}$.
   После этих преобразований получим
   $$
   \begin{gathered}
   a_1^* = \beta_{11}b_1 + ... + \beta_{1 \ q-1}b_{q-1}; \\
   ... \\
   a^*_{p-1} = \beta_{p-1 \ 1}b_1 + ... + \beta_{p - 1 \ q - 1}b_{q - 1}.
   \end{gathered} \ \ \ \ (2.2)
   $$
   $$
   a^*_i = a_i - \frac{\alpha_{iq}}{\alpha_{pq}}a_p, \ \ \ \ (1\leq i \leq p - 1). \ \ \ \ \ \ (2.3)
   $$
   Рассмотрим системы векторов:
   $$
   \begin{gathered}
   a^*_{1}, ..., a^*_{p-1} \ \ (I^*) \\
   b_{1}, ..., b_{q-1} \ \ (II*)
   \end{gathered}
   $$
   Равенства $(2.2)$ указывают на то, что система $(I^*)$ линейно выражается через систему $(II^*)$. Покажем, что система $(I^*)$ является ЛНЗ. По определению пусть
   $$
   \alpha_1a^*_{1} + ... + \alpha_{p-1}a_{p-1}^* = 0.
   $$
   Получим,
   $$
   \alpha_1 (a_1 - \frac{\alpha_{1q}}{\alpha_{pq}}) + ... + \alpha_{p-1} (a_{p-1}-\frac{\alpha_{p-1 \ q}}{\alpha_{pq}}a_p) = 0.
   $$
   $$
   \alpha_1a_1 + ... + \alpha_{p-1}a_{p-1} + \alpha a_p = 0.
   $$
   Так как система $(I)$ является ЛНЗ, то из последнего равенства следует
   $$
   \alpha_1 = ... = \alpha_{p-1} = \alpha = 0
   $$
   Следовательно, система $(I^*)$ является ЛНЗ. Тогда применяем предположение индукции, что число векторов в $(I^*)$ не превосходит число векторов в $(II^*)$, то есть  $p - 1 \leq q - 1 \Rightarrow p \leq q$.

***Теорема*** *(о максимальном числе ЛНЗ векторов в $k^n$)*
В $n$-мерном координатном линейном пространстве $k^n$ существуют ЛНЗ системы, состоящие из $n$ векторов, через которые линейно выражаются все векторы пространства $k^n$. Любая система векторов пространства $k^n$, содержащая более $n$ векторов, является ЛЗ.

*Доказательство*:
Рассмотрим так называемую систему единичных векторов пространства $k^n$.
$$
e_1=(1, 0, ..., 0), \ e_2=(0, 1, ..., 0), \ ..., \ e_n=(0, 0, ..., 1),
$$
Видно, что этих векторов $n$ штук, докажем, что они являются ЛНЗ. В самом деле, пусть
$$
\begin{gathered}
\alpha_1e_1+\alpha_2e_2 + ... + \alpha_ne_n = 0, \\
(\alpha_1, 0, ..., 0) + (0, \alpha_2, ..., 0) + ... + (0, 0, ..., \alpha_n) = 0.
\end{gathered}
$$
Получается $(\alpha_1, \alpha_2, ..., \alpha_n) = 0$. $(\forall \ 1 \leq i \leq n) \ \ \alpha_i = 0$. Следовательно, $e_1, e_2, ..., e_n$ является ЛНЗ системой векторов. Покажем, что все векторы в пространстве $k^n$ линейно выражаются через $e_1, e_2, ..., e_n$. В самом деле, пусть вектор $b=(\beta_1, \beta_2, ..., \beta_n)$ - любой вектор из $k^n$. Тогда
$$
b = (\beta_1, 0, ..., 0) + (0, \beta_2, ..., 0) + ... + (0, 0, ..., \beta_n) = \beta_1e_1 + \beta_2e_2+...+\beta_ne_n.
$$
Пусть $\alpha_1, \alpha_2, ..., \alpha_p$ - любая ЛНЗ система векторов из $k^n$. Тогда все эти векторы линейно выражаются через $e_1, e_2, ..., e_n$. Тогда по основной теореме о линейной зависимости, число векторов $(I)$ не превосходит число векторов во второй системе, то есть $p \leq n$. Мы видим, что в любой ЛНЗ системе векторов пространства $k^n$ должно содержаться не более $n$ векторов. Следовательно, если система содержит более $n$ векторов, она должна быть ЛЗ.

***Определение***:
ЛНЗ система векторов $a_1, a_2, ..., a_s$ называется максимальной ЛНЗ подсистемой системы векторов $A$, если добавление ней любого вектора $a \in A$ превращает её в ЛЗ систему.
Любая ЛНЗ система, состоящая из $n$ векторов является максимальной ЛНЗ подсистемой пространства $k^n$.

---

# Базис и ранг системы векторов
***Определение***:
Говорят, что система векторов $A$ линейно выражается через систему векторов $B$, если каждый вектор $a \in A$ является линейной комбинацией бесконечного подмножества векторов системы $B$.
$$
(\forall a \in A) \ \ a=\sum_{b\in B}{ a_bb},
$$
где почти все $a_b = 0$ (то есть только конечное множество $a_b \not= 0$).

Если $A\subset B$, то $A$ линейно выражается через $B$. Действительно, в это случае
$$
(\forall \ a \in A) \ \ a = 1 \cdot a + 0 \cdot b_1 + 0 \cdot b_2 + ... = \sum_{b \ in B}{a_bb}.
$$
Понятие линейного выражения одной системы через другую обладает свойством транзитивности, а именно, если $A$ линейно выражается через $B$, а $B$ линейно выражается через $C$, то $A$ линейно выражается через $C$.

***Определение***:
Две системы векторов $A \ и \ B$ называются эквивалентными $(A \thicksim B)$, если они линейно выражаются через друг друга.

*Свойства эквивалентности*:
1. $A \thicksim A$ (рефлексивность)
2. Если $A \thicksim B, \text{ то } B \thicksim A$ (симметричность)
3. Если $A \thicksim B, B \thicksim C, \text{ то } A \thicksim C$ (транзитивность).


***Теорема*** (о равносильных условиях, определяющий базис):
Пусть $B$ линейно независимая подсистема системы $A$. тогда равносильны следующие утверждения:
1. Системы векторов $A$ линейно выражаются через подсистему $B$.
2. Система векторов $(B, a)$, получающаяся присоединением к $B$ любого вектора $a \in A$, является линейно зависимой.
3. В системе векторов $A$ не существует линейно независимых подсистем с числом векторов большим, чем в $B$.

*Доказательство*:
Прежде всего заметим, что если система $A$ может быть как конечной, так и бесконечной, то система $B$ обязательно должна быть конечной.

*Покажем, что из утверждения 1 следует 2.*
Возьмём любой вектор $a \in A$. Составим $(B, a)$. Так как вектор $a$ линейно выражается через $B$, то по критерию линейной зависимости эта система $(B, a)$ является линейно зависимой.
Выполнение условия 2 означает, что $B$ является максимальной линейно независимой подсистемой системы $A$.

*Покажем, что из утверждения 2 следует 3*.
Пусть $B'$ - любая линейно независима подсистема векторов из $A$. Возьмём $a \in B'$ и составим $(B, a)$. По условию 2 она является линейно зависимой, тогда по свойству 3 линейной зависимости векторов $a$ выражается через $B$. Следовательно $B'$ линейно выражается через $B$. Так как $B'$ - линейно независима, то по основной теореме о линейной зависимости число векторов в $B'$ не превосходит числа векторов из $B'$.

*Покажем, что из утверждения 3  следует 1*.
Возьмём любой вектор $a \in A$ и составим $(B, a)$. Так как в этой системе число векторов больше, чем в $B$, то по условию 3 она является линейно зависимой, отсюда по свойству 3 линейной зависимости, вектор $a$ линейно выражается через $B$. Таким образом вся система $A$ линейно выражается через $B$.


***Определение***:
Базисом системы векторов $A$ называется любая её линейно независимая подсистема $B$, удовлетворяющая любому из равносильных условий теоремы о равносильных условиях, определяющих базис.

***Определение***:
Базисом системы векторов $A$ называется такая её линейно независимая подсистема $B$, через которую выражаются все векторы системы $A$.

*Определение*:
Базисом системы векторов $A$ называется любая её максимальная линейно независимая подсистема.

### Свойства базисов
1. *Система векторов $A$ эквивалентна любому своему базису $B$.*
   Действительно, $A$ линейно выражается через $B$ (по определению) с другой стороны, та как $B \subset A$, то $B$ линейно выражается через $A$. Тогда по определению эквивалентной системы: $A \thicksim B$

2. Количество векторов во всех базисах системы векторов $A$ одно и то же.
   Пусть $B \ и \ B'$ - любые два базиса системы векторов $A$. Тогда, с одно стороны, $B'$ - линейно независимое и линейно выражается через $B$. Отсюда по основной теореме о линейной зависимости следует, что число векторов $B'$ равно числу векторов в $B$.

### Ранг системы векторов
***Определение***:
Рангом системы векторов называется число векторов в любом базисе этой системы.

***Определение***:
Рангом системы векторов называется максимальное число её линейно независимых векторов.

### Свойства рангов
1. Если система векторов  $A$ линейно выражается через систему векторов $B$, то ранг системы $A$ меньше или равен рангу системы $B$, то есть $r(A) \leq r(B)$, где $r$ - ранг.
   Пусть $A'$ - базис $A$, $B'$ - базис $B$. Тогда $A'$ линейно выражается через $A$, $A$ ли выражается через $B$ (по условию), $B$ линейно выражается через $B'$. Кроме того, $A'$ линейно независимая система векторов, тогда число векторов в $A'$ меньше или равно числу векторов в $B'$, то есть $r(A) \leq r(B)$.

 2. Ранги эквивалентных систем векторов равны
    Действительно, пусть $A \thicksim B$. тогда с одной стороны, $A$ линейно выражается через $B$ и по свойству 1: $r(A) \leq r(B)$. С другой стороны, $B$ линейно выражается через $A$, тогда по свойству 1: $r(B) \leq r(A)$, Из этих двух неравенств следует, что $r(A) = r(B)$.

---
# Ранг матрицы
***Определение***:
Прямоугольной матрицей $n \times m$ называется прямоугольная матрица из $n \times m$ чисел $a_{ij}$, записанная в виде $n$ строк и $m$ столбцов
$$
A= \begin{pmatrix}
a_{11} && a_{12} && ... && a_{1m} \\
a_{21} && a_{22} && ... && a_{2m} \\
... && ... && ... && ... \\
a_{n1} && a_{n2} && ... && a_{nm}
\end{pmatrix}.
$$
На столбцы матрицы можно смотреть как на векторы пространства $k^n$. На строки матрицы можно смотреть как на векторы пространства $k^m$.

***Определение***:
Рангом матрицы называется ранг системы векторов, образованной её столбцами.

***Определение***:
Рангом матрицы называется максимальное число её независимых столбцов.

***Определение***:
Пусть дана матрица $A_{n\times m}$ и максимальное число $1 \leq k \leq min(n, m)$. Минором $k$-порядка матрицы $A$ называется определитель, порождённый матрицей, состоящей из элементов, стоящих на пересечении любых  $k$ строк и любых $k$ столбцов матрицы $A$.



***Теорема*** (О ранге матрицы):
Ранг матрицы равен наивысшему порядку отличных от нуля миноров этой матрицы.

*Доказательство*:
Пусть наивысший порядок, отличных от нуля миноров матрицы $A$ равен $r$. Без ограничения в общности будем считать, что не равный нулю минор $M$ порядка $r$ находится в левом верхнем углу матрицы $A$
$$
A = \begin{pmatrix}
a_{11} && ... && a_{1r} && ... && a_{1m} \\
... && ... && ... && ...&& ... \\
a_{r1} && ... && a_{rr} && ... && a_{rm} \\
... && ... && ... && ... && ... \\
a_{n1} && ... && a_{nr} && ... && a_{nm}
\end{pmatrix}, M \not= 0.
$$
1. Покажем, что первые $r$ столбцов матрицы $A$ являются линейно независимыми. 
   Допустим противное, ито есть первые $r$ столбцов матрицы являются линейно зависимыми, тогда по критерию линейной зависимости, по крайней мере один столбец, например первый, линейно выражается через остальные $(r-1)$ столбцов. Переходя от столбцов матрицы $A$ к столбцам минора $M$, мы получим, что первый столбец минора $M$ линейно выражается через остальные $(r-1)$ столбцов. А тогда по свойству 8 определителей $M=0$, а это противоречит тому, что $M\not = 0$. Следовательно, первые $r$ столбцов матрицы $A$ линейно независимы
2. Покажем, что любой $l$-столбец, где $r \leq l \leq n$, линейно выражается через первые $r$ столбцов матрицы $A$.
   Зафиксируем $i, \ 1 \leq i \leq n$, и рассмотрим $n$ определителей $\Delta_i$, получающиеся приписыванием к минору $M$ элементов $i$-строки и $l$-столбца матрицы $A$
   $$
   \Delta_i = \begin{vmatrix}
   a_{11} && ... && a_{1r} && a_{1l} \\
   ... && ... && ... && ... \\
   a_{r1} && ... && a_{rr} && a_{rl} \\
   a_{i1} && ... && a_{ir} && a_{il}
   \end{vmatrix}.
   $$
Покажем, что все определители $\Delta_i = 0$. 
Действительно, если $1 \leq i \leq r$, то $\Delta_i = 0$, так как он содержит две одинаковые строки. Если $r < i \leq n$, то $\Delta_i$ будет минором $(r+1)$-порядка матрицы $A$. Тогда он будет равен нулю, в силу выбора числа $r$. Значит, все $\Delta_i = 0$ при $1 \leq i \leq n$.
Разложим определитель $\Delta_i$ по элементам последней строки. Получим,
   $$
   \Delta_i = a_{i1}A_{i1} + a_{i2}A_{i2} + ... + a_{ir}A_{ir} + a_{il}M = 0,
   $$
где $M$, $A_{i1}, A_{i2}, ..., A_{ir}$ - не зависят от $i$. Следовательно, 
   $$
   a_{il}=(-\frac{A_{i1}}{M})a_{i1} + (-\frac{A_{i2}}{M})a_{i2} + ... + (-\frac{A_{ir}}{M})a_{ir}.
   $$
Это равенство справедливо для всех $1 \leq i \leq n$, при коэффициентах
   $$
   -\frac{A_{i1}}{M_1}, -\frac{A_{i2}}{M_2}, ..., -\frac{A_{ir}}{M},
   $$
не зависящих от $i$.
Переписывая равенство для всех $i$, мы получим, что $l$-столбец матрицы $A$ линейно выражается через первые $r$-столбцов этой матрицы с коэффициентами
   $$
   -\frac{A_{i1}}{M}, -\frac{A_{i2}}{M}, ..., -\frac{A_{ir}}{M}.
   $$
Из 1. и 2. следует, что первые $r$-столбцов матрицы $A$ образуют базис всех столбцов этой матрицы $A$, поэтому $r(A)=r$.

***Следствие***:
Максимальное число линейно независимых строк матрицы равно максимальному числу её линейно независимых столбцов, то есть равно рангу матрицы.

*Доказательство*:
Рассмотрим матрицы $A \ и \ A'$ (транспонированная матрица). Ясно, что если матрица $A$ содержит минор $M$, то матрица $A'$ содержит минор $M'$. Верно и обратное. При транспонировании величина определителя не меняется. Из всего сказанного следует, что максимальные порядки, отличных от нуля миноров матриц $A$ и $A'$ совпадают. тогда по теореме о ранге матрицы $r(A')=r(A)$. Таким образом получаем следующую цепочку равенств: максимальное число ЛНЗ столбцов матрицы $A$ равно $r(A) = r(A')$ равно максимальному числу ЛНЗ столбцов матрицы $A'$ равно максимальному числу линейно независимых строк матрицы $A$.


***Следствие*** (Критерий равенства нулю определителя):
Определитель равен нулю тогда и только тогда, когда его строки линейно зависимы.

*Доказательство*:
1. Достаточность
   Пусть строки определителя $\Delta$ являются линейно зависимыми. Тогда по критерию линейной зависимости хотя бы одна их строк линейно выражается через другие его строки, а тогда по 8 свойству определителей $\Delta = 0$.
2. Необходимость
   Пусть определитель $\Delta = 0$. Рассмотрим матрицу $A$ этого определителя. Её единственный минор $n$-порядка $\Delta=0$. По теореме о ранге матрицы: $r(A) < n$. Это означает, что максимальное число линейно независимых строк матрицы $A$ меньше $n$, тогда все строки матрицы $A$ ($n$-штук) должны быть линейно зависимыми.

